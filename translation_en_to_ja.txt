{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "translation_en_to_ja.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jK88JoQ79a_G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12fef5c0-d2f5-483b-856f-ea0cbf2cbcc5"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, Input, dot, Activation, Concatenate\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import initializers, regularizers, constraints\n",
        "import time\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "!pip install mojimoji\n",
        "!pip install sentencepiece\n",
        "import mojimoji\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "\n",
        "import nltk\n",
        "import unicodedata\n",
        "import sentencepiece as spm\n",
        "\n",
        "# ignore warning\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "print(tf.__version__)\n",
        "# gpu\n",
        "tf.test.is_gpu_available() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mojimoji in /usr/local/lib/python3.7/dist-packages (0.0.11)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n",
            "2.5.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 529
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjJ2jflk5IT3"
      },
      "source": [
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "                 if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "  w = w.strip()\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_L_ygdR9a_Q"
      },
      "source": [
        "# load text file\n",
        "\n",
        "this dataset is aleady implemented a SentenceSpace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYlzMp1U9a_T"
      },
      "source": [
        "num_example = 300000\n",
        "\n",
        "# create each languages list\n",
        "def create_lang_list(num_example):\n",
        "    # load txt file\n",
        "    lines =  io.open(\"corpus.txt\", encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_example]]\n",
        "\n",
        "    return zip(*word_pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qG4dRId39a_X"
      },
      "source": [
        "en, ja = create_lang_list(num_example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMlaIsar9bAf"
      },
      "source": [
        "# tokenize\n",
        "tokenize each language word based on space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACN_sW7u9bAh"
      },
      "source": [
        "def tokenize(lang):\n",
        "    # vectorize a text corpus\n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        filters=' ')\n",
        "\n",
        "    # updates internal vocabulary based on a list of texts\n",
        "    # e.g. \"[this place is good ]\"→{this:2, place:3, is:1, good:4} \"\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "    # Transforms each text in texts to a sequence of integers.\n",
        "    # e.g. this place is good → [[2, 3, 1, 4]]\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "    # transform a list of num sample into a 2D Numpy array of shape \n",
        "    # Fixed length because length of sequence of integers are different\n",
        "    # return (len(sequences), maxlen)\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                          padding='post')\n",
        "    return tensor, lang_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDfQw_5t9bA5"
      },
      "source": [
        "# create clean dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIM8-10h9bA6"
      },
      "source": [
        "# cleate a clean dataset\n",
        "def create_dataset(en, ja):\n",
        "    \n",
        "    # input_tensor, target_tensor: 2d numpy array\n",
        "    # input_lang_tokenize, target_lang_tokenize: word dictionary\n",
        "    input_tensor, input_lang_tokenize = tokenize(en)\n",
        "    target_tensor, target_lang_tokenize = tokenize(ja)\n",
        "\n",
        "    return input_tensor, target_tensor, input_lang_tokenize, target_lang_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEJJ9BMgATdD"
      },
      "source": [
        "input_tensor, target_tensor, input_lang_tokenize, target_lang_tokenize = create_dataset(en, ja)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzlBqTlO9bA-"
      },
      "source": [
        "def max_length(input_tensor, target_tensor):\n",
        "\n",
        "    # max length of input sentense and target sentense\n",
        "    english_len = [len(i) for i in input_tensor]\n",
        "\n",
        "    japanese_len = [len(i) for i in target_tensor]\n",
        "\n",
        "     # print max length\n",
        "    print(\"english length:\", max(english_len))\n",
        "    print(\"japanese length:\", max(japanese_len))\n",
        "    max_len_input =  max(english_len)\n",
        "    max_len_target =  max(japanese_len)\n",
        "\n",
        "    return max_len_input, max_len_target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ew-q24gJ9bBB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33c3ed48-3823-4eb6-8b59-6af16096eb39"
      },
      "source": [
        "# Calculate max_length of the target tensors\n",
        "max_length_input, max_length_target = max_length(input_tensor, target_tensor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "english length: 61\n",
            "japanese length: 60\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_HvHj059bBG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bdfde88-ae07-498d-f02e-dc4752c1e245"
      },
      "source": [
        "# create trainnig set and validation set\n",
        "X_train, X_test, \\\n",
        "    Y_train, Y_test = train_test_split(input_tensor, target_tensor, test_size=0.2, shuffle=True)\n",
        "\n",
        "X_test, X_val, \\\n",
        "    Y_test, Y_val = train_test_split(X_test, Y_test, test_size=0.5, shuffle=True)\n",
        "\n",
        "\n",
        "# show length\n",
        "print(len(X_train), len(Y_train), len(X_test), len(Y_test), len(X_val), len(Y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "609 609 76 76 77 77\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w12Za_JK9bBJ"
      },
      "source": [
        "def convert(lang, tensor):\n",
        "    for t in tensor:\n",
        "        if t != 0:\n",
        "            # Index number assigned to each word\n",
        "            print(\"%d----->%s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6yT2odT9bBP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4b447bb-c43d-4436-bf5f-92be9671d238"
      },
      "source": [
        "print(\"input lang: index to word mapping\")\n",
        "convert(input_lang_tokenize, X_train[10])\n",
        "print(\"output lang: index to word mapping\")\n",
        "convert(target_lang_tokenize, Y_train[10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input lang: index to word mapping\n",
            "1-----><start>\n",
            "681----->annee\n",
            "3----->.\n",
            "2-----><end>\n",
            "output lang: index to word mapping\n",
            "1-----><start>\n",
            "129----->mbula\n",
            "3----->.\n",
            "2-----><end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSHdoFwQ9bBT"
      },
      "source": [
        "# define parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jhj29n1l9bBT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56b4806f-a8f7-4b9b-ff15-43f34ef96556"
      },
      "source": [
        "# BUFFER_SIZE >= dataset if smaller than dataset can't shuffle equally\n",
        "BUFFER_SIZE = len(X_train)\n",
        "BATCH_SIZE = 32\n",
        "dropout_rate = 0.3\n",
        "# if None steps_per_epoch == mum of dataset\n",
        "train_steps_per_epoch = len(X_train) // BATCH_SIZE\n",
        "val_steps_per_epoch = len(X_val) // BATCH_SIZE\n",
        "print(\"train step %d\" % train_steps_per_epoch)\n",
        "embedding_dim = 300\n",
        "units = 512\n",
        "vocab_inp_size = len(input_lang_tokenize.word_index) + 1\n",
        "print('Total unique words in the input: %s' % len(input_lang_tokenize.word_index))\n",
        "print('Total unique words in the target: %s' % len(target_lang_tokenize.word_index))\n",
        "vocab_tar_size = len(target_lang_tokenize.word_index) + 1\n",
        "\n",
        "# create train dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# create validation dataset\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, Y_val)).batch(BATCH_SIZE, drop_remainder=True)\n",
        "example_input_batch, example_target_batch = next(iter(train_dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train step 19\n",
            "Total unique words in the input: 1353\n",
            "Total unique words in the target: 1157\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([32, 61]), TensorShape([32, 60]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 541
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjAVocfI9bCI"
      },
      "source": [
        "# Encoder Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJWSAHhl9bCJ"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size, dropout_rate):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.enc_units = enc_units\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.first_lstm = tf.keras.layers.LSTM(self.enc_units,\n",
        "                                                            return_sequences=True,\n",
        "                                                            recurrent_initializer='glorot_uniform')\n",
        "        \n",
        "        self.final_lstm = tf.keras.layers.LSTM(self.enc_units,\n",
        "                                                    return_sequences=True,\n",
        "                                                    return_state=True,\n",
        "                                                    recurrent_initializer='glorot_uniform')\n",
        "        \n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.first_lstm(x, initial_state =hidden)\n",
        "        output, state_h, state_c = self.final_lstm(x)\n",
        "        state = [state_h, state_c ]\n",
        "\n",
        "        return output, state\n",
        "        \n",
        "    def initialize_hidden_state(self):\n",
        "            return tf.zeros((self.batch_size , self.enc_units)), tf.zeros((self.batch_size , self.enc_units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V53dyDZO_F35"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE, dropout_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSn1VowJ9bCS"
      },
      "source": [
        "# attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cctmI1mA9bCS"
      },
      "source": [
        "class Attention(tf.keras.models.Model):\n",
        "\n",
        "    def __init__(self, units: int, *args, **kwargs):\n",
        "\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.units = units\n",
        "\n",
        "        self.q_dense_layer = Dense(units, use_bias=False, name='q_dense_layer')\n",
        "        self.k_dense_layer = Dense(units, use_bias=False, name='k_dense_layer')\n",
        "        self.v_dense_layer = Dense(units, use_bias=False, name='v_dense_layer')\n",
        "        self.output_dense_layer = Dense(units, use_bias=False, name='output_dense_layer')\n",
        "\n",
        "    def call(self, input, memory):\n",
        "\n",
        "        q = self.q_dense_layer(input) \n",
        "        k = self.k_dense_layer(memory) \n",
        "        v = self.v_dense_layer(memory)\n",
        "\n",
        "        depth = self.units // 2\n",
        "        q *= depth ** -0.5  # for scaled dot production\n",
        "\n",
        "        # caluclate relation between query and key\n",
        "        logit = tf.matmul(q, k, transpose_b=True) \n",
        "\n",
        "        attention_weight = tf.nn.softmax(logit)\n",
        "\n",
        "        attention_output = tf.matmul(attention_weight, v) \n",
        "        return self.output_dense_layer(attention_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBBPzDxK9bCg"
      },
      "source": [
        "# Decoder Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nED5gCZ9bCh"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_size, dropout_rate):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "        self.first_lstm = tf.keras.layers.LSTM(self.dec_units,\n",
        "                                                            return_sequences=True)\n",
        "        self.final_lstm = tf.keras.layers.LSTM(self.dec_units,\n",
        "                                                            return_sequences=True,\n",
        "                                                            return_state=True)\n",
        "                                                            \n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "        self.attention = Attention(self.dec_units)\n",
        "    \n",
        "    def call(self, x, hidden, enc_output):\n",
        "        x = self.embedding(x)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        x =  self.first_lstm(x)\n",
        "        output, state_h, state_c = self.final_lstm(x)\n",
        "        state = [state_h, state_c]\n",
        "        attention_weights = self.attention(output, enc_output)\n",
        "        output = tf.concat([output, attention_weights], axis=-1)\n",
        "\n",
        "                \n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        \n",
        "        output = self.fc(output)\n",
        "        \n",
        "        return  output, state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37-lrVTN_F4X"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, dropout_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvyXf1bn9bC1"
      },
      "source": [
        "# optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wROZueWU9bC3"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.9, epsilon=1e-04, decay=1e-06)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtDLuqB49bC7"
      },
      "source": [
        "# Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CWEvq3W9bC8"
      },
      "source": [
        "checkpoint_dir = './train_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja0mhQ4j9bDB"
      },
      "source": [
        "# train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQSV9kkd9bDB"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([target_lang_tokenize.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "    #dec_input = tf.expand_dims([target_lang_tokenize.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the sladecoder\n",
        "      predictions, dec_hidden = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss\n",
        "\n",
        "  print(batch_loss) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCI9q4-C9bDI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccc8c70c-991d-4816-9a23-6ed28130c1cd"
      },
      "source": [
        "# trained model for 45 epochs\n",
        "EPOCHS = 15\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  \n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  train_loss = 0\n",
        "  val_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(train_dataset.take(train_steps_per_epoch)):\n",
        "    train_batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    train_loss += train_batch_loss\n",
        "\n",
        "\n",
        "  for (batch, (val_inp, val_tar)) in enumerate(val_dataset.take(val_steps_per_epoch)):\n",
        "    val_batch_loss = train_step(val_inp, val_tar, enc_hidden)\n",
        "    val_loss += val_batch_loss\n",
        "    if batch % 100 == 0:\n",
        "      print(f'Epoch {epoch+1} Batch {batch} Loss {val_batch_loss.numpy():.4f}')\n",
        "\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "\n",
        "  print(f'Epoch {epoch+1} Loss {val_loss/val_steps_per_epoch:.4f}')\n",
        "  print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 0.5984\n",
            "Epoch 1 Loss 0.7148\n",
            "Time taken for 1 epoch 4531.64 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.5256\n",
            "Epoch 2 Loss 0.6304\n",
            "Time taken for 1 epoch 4537.29 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.4906\n",
            "Epoch 3 Loss 0.5972\n",
            "Time taken for 1 epoch 4542.75 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.4728\n",
            "Epoch 4 Loss 0.5745\n",
            "Time taken for 1 epoch 4548.33 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.4677\n",
            "Epoch 5 Loss 0.5621\n",
            "Time taken for 1 epoch 4553.72 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.4457\n",
            "Epoch 6 Loss 0.5405\n",
            "Time taken for 1 epoch 4559.44 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.4355\n",
            "Epoch 7 Loss 0.5249\n",
            "Time taken for 1 epoch 4564.96 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.4286\n",
            "Epoch 8 Loss 0.5149\n",
            "Time taken for 1 epoch 4570.69 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.4280\n",
            "Epoch 9 Loss 0.5156\n",
            "Time taken for 1 epoch 4576.30 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.4169\n",
            "Epoch 10 Loss 0.4979\n",
            "Time taken for 1 epoch 4582.14 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.4138\n",
            "Epoch 11 Loss 0.4886\n",
            "Time taken for 1 epoch 4587.72 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.4028\n",
            "Epoch 12 Loss 0.4781\n",
            "Time taken for 1 epoch 4593.51 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.3983\n",
            "Epoch 13 Loss 0.4681\n",
            "Time taken for 1 epoch 4598.98 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.3891\n",
            "Epoch 14 Loss 0.4650\n",
            "Time taken for 1 epoch 4604.65 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.3812\n",
            "Epoch 15 Loss 0.4529\n",
            "Time taken for 1 epoch 4610.14 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOhXwpZlDwJp"
      },
      "source": [
        "# Load trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVDzzIKJBsBU"
      },
      "source": [
        "#checkpoint.restore(\"train_model/ckpt-5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQnK48R0BuZT"
      },
      "source": [
        "# into base model\n",
        "encoder = checkpoint.encoder\n",
        "decoder = checkpoint.decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gu5bPpdEHVz"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "Check bleu score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJ8r9JNREs01"
      },
      "source": [
        "def predict(sentence):\n",
        "    inputs = tf.convert_to_tensor(sentence)\n",
        "    result = ''\n",
        "    inputs = tf.expand_dims(inputs, axis=0)\n",
        "    hidden = [tf.zeros((1, units)), tf.zeros((1, units))]\n",
        "    enc_out, state = encoder(inputs, hidden)\n",
        "    hidden_state = state\n",
        "    dec_input = tf.expand_dims([target_lang_tokenize.word_index['<start>']], 0)\n",
        "    for t in range(max_length_target):\n",
        "        predictions, hidden_state = decoder(dec_input,\n",
        "                                                             hidden_state,\n",
        "                                                             enc_out)\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += target_lang_tokenize.index_word[predicted_id] + ' '\n",
        "        if target_lang_tokenize.index_word[predicted_id] == '_end' or len(result) > max_length_target:\n",
        "            return result\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87pBzvXjRkJL"
      },
      "source": [
        "def create_reference(lang, tensor):\n",
        "    all_sentence_list = []\n",
        "\n",
        "    for word_list in tensor:\n",
        "      sentence_list = []\n",
        "\n",
        "      for t in word_list:\n",
        "          if not t == 0:\n",
        "              # Index number assigned to each word\n",
        "              sentence_list.append(lang.index_word[t])\n",
        "      all_sentence_list.append(sentence_list)\n",
        "    return all_sentence_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGwwlmHtRrfb"
      },
      "source": [
        "# create reference\n",
        "reference = create_reference(target_lang_tokenize, Y_test.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAAMX-vQEKB_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "895229d3-8f38-42c0-8a3d-26b5841be2fa"
      },
      "source": [
        "from tqdm import tqdm\n",
        "# create hypothesis\n",
        "hypothesis = []\n",
        "for i in tqdm(X_test):\n",
        "  hypothesis.append(predict(i))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 76/76 [00:13<00:00,  5.57it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-i0Jl_21Ee7k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7e390f0-8d50-4861-f617-717a35111eef"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "score = 0\n",
        "smoothie = SmoothingFunction().method2\n",
        "for i in range(len(reference)):\n",
        "    score += sentence_bleu([reference[i][1:-1]], hypothesis[i][:-5].strip().split(), smoothing_function=smoothie)\n",
        "\n",
        "score /= len(reference)\n",
        "print(\"The bleu score is: \"+str(score))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The bleu score is: 0.08005696822990507\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNkXiemq9bDO"
      },
      "source": [
        "# Translation example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-DcisWA9bDP"
      },
      "source": [
        "#def preprocess_sentence(en_text):\n",
        "        # normalize English\n",
        "        #en_text = en_text.lower()\n",
        "        #en_text = english_unicode_to_ascii(en_text)\n",
        "        #en_text = replace_special_character_to_space_en(en_text)\n",
        "\n",
        "        #en_text = \"start_ \" + en_text + \" _end\"\n",
        "\n",
        "        #return en_text\n",
        "        \n",
        "def evaluate(sentence):\n",
        "  \n",
        "    attention_plot = np.zeros((max_length_target, max_length_input))\n",
        "\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    inputs = [input_lang_tokenize.word_index[i] for i in sentence.split(' ')]\n",
        "\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                           maxlen=max_length_input,\n",
        "                                                           padding='post')\n",
        "    \n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    result = ''\n",
        "    hidden = [tf.zeros((1, units)), tf.zeros((1, units))]\n",
        "    enc_out, state = encoder(inputs, hidden)\n",
        "    hidden_state = state\n",
        "    dec_input = tf.expand_dims([target_lang_tokenize.word_index['<start>']], 0)\n",
        "    for t in range(max_length_target):\n",
        "        predictions, hidden_state = decoder(dec_input,\n",
        "                                                             hidden_state,\n",
        "                                                             enc_out)\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += target_lang_tokenize.index_word[predicted_id] + ' '\n",
        "        if target_lang_tokenize.index_word[predicted_id] == '_end' or len(result) > max_length_target:\n",
        "            return result, sentence\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    return result, sentence, "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ui6bFxRt9bDd"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def result(sentence):\n",
        "    result, sentence = evaluate(sentence)\n",
        "\n",
        "    return result, sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2duMXKX9bDh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d3f3376-1303-4868-c244-b0af8846329c"
      },
      "source": [
        "result, sentence = result(\"médicaments\")\n",
        "print('Input: %s' % (sentence))\n",
        "print('Predicted translation: {}'.format(result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> medicaments <end>\n",
            "Predicted translation: na yo . <end> . <end> . <end> . <end> . <end> . <end> . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlCu-uvtwOft",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "80c8c670-d5e7-4708-f060-d98115c7383b"
      },
      "source": [
        "result, sentence = result(\"je suis malade\")\n",
        "print('Input: %s' % (sentence))\n",
        "print('Predicted translation: {}'.format(result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-561-88a4f989e8c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"je suis malade\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Predicted translation: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2FZwaiky-yj"
      },
      "source": [
        "result, sentence = result(\"médicaments\")\n",
        "print('Input: %s' % (sentence))\n",
        "print('Predicted translation: {}'.format(result))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67d09FoPz2Al"
      },
      "source": [
        "result, sentence = result(\"je suis un médécin\")\n",
        "print('Input: %s' % (sentence))\n",
        "print('Predicted translation: {}'.format(result))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2saSpKK0Cwl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}